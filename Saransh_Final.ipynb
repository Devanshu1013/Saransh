{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53e9168",
   "metadata": {},
   "source": [
    "nltk: Natural Language Toolkit, a powerful library for working with human language data.\n",
    "stopwords: A list of common words (e.g., \"the\", \"is\", \"and\") that are often removed in text analysis because they don't carry much information.\n",
    "WordNetLemmatizer: Used for lemmatization, which reduces words to their base or root form.\n",
    "sent_tokenize and word_tokenize: Functions for tokenizing text into sentences and words, respectively.\n",
    "Counter: A class for counting the occurrences of elements in a collection (useful for frequency analysis).\n",
    "translate: A library for translating text.\n",
    "pyttsx3: A library for text-to-speech conversion.\n",
    "fitz: PyMuPDF, a library for working with PDF files.\n",
    "os: Provides a way of interacting with the operating system, used for file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3bb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from translate import Translator as TranslateLib\n",
    "import pyttsx3\n",
    "import fitz\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21714dc7",
   "metadata": {},
   "source": [
    "It will download the stopwords, wordnet, and punkt form the nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd075415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab166b9f",
   "metadata": {},
   "source": [
    "Here, this function will count the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccccbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(input_text):\n",
    "    words = input_text.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f6b66",
   "metadata": {},
   "source": [
    "This function will read the pdf and gives the text of the pdf and the pages. Moreover it will take the starting page and the ending page then after it will print the number of total pages and number of words and then it will print the text from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977cf12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path, start_page=0, end_page=None):\n",
    "    # Read text from specific pages of a PDF file using PyMuPDF\n",
    "    doc = fitz.open(file_path)\n",
    "    \n",
    "    # Determine the end page\n",
    "    if end_page is None or end_page > doc.page_count:\n",
    "        end_page = doc.page_count\n",
    "    \n",
    "    text = \"\"\n",
    "    for page_num in range(start_page, end_page):\n",
    "        page = doc[page_num]\n",
    "        text += page.get_text()\n",
    "    \n",
    "    # Close the PDF document\n",
    "    doc.close()\n",
    "    \n",
    "    return text, end_page\n",
    "\n",
    "\n",
    "pdf_file_path = 'Docs/Research.pdf'\n",
    "start_page = 0  # Specify the starting page\n",
    "end_page = 5    # Specify the ending page (set to None if you want to process all pages)\n",
    "\n",
    "text, total_pages = read_pdf(pdf_file_path, start_page, end_page)\n",
    "print(f\"Total number of pages in the PDF: {total_pages}\")\n",
    "\n",
    "Text_Wrods = count_words(text)\n",
    "print(f\"Number of words in the input: {Text_Wrods}\")\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe6944",
   "metadata": {},
   "source": [
    "This will take the text input and count the words present in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd80c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# text = \"\"\"\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# Text_Count = count_words(text)\n",
    "# print(f\"Number of words in the input: {Text_Count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b96a9d3",
   "metadata": {},
   "source": [
    "Here, the python text to speech will make the audio file of the inout text and it will be saved in the audio folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00dac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = pyttsx3.init()\n",
    "\n",
    "# # Set properties (optional)\n",
    "# engine.setProperty('rate', 150)  # Speed of speech\n",
    "\n",
    "# Convert the text to audio and save it as an MP3 file\n",
    "# input_pdf_audio_file = 'input_pdf_audio.mp3'\n",
    "input_pdf_audio_file = 'Audios/1.Main_Text.wav'\n",
    "engine.save_to_file(text, input_pdf_audio_file)\n",
    "\n",
    "# Wait for the audio to finish playing\n",
    "engine.runAndWait()\n",
    "\n",
    "# Runs the audio File\n",
    "# os.system(f\"start {input_pdf_audio_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c8661f",
   "metadata": {},
   "source": [
    "Create Lemmatizer and Stop Words Set:\n",
    "An instance of the WordNetLemmatizer is created, which will be used to lemmatize words in the text.\n",
    "The set of English stop words is obtained using stopwords.words('english').\n",
    "\n",
    "Tokenize Text:\n",
    "The input text is tokenized into sentences using sent_tokenize.\n",
    "Each sentence is further tokenized into words using word_tokenize.\n",
    "\n",
    "Lemmatize Words and Remove Stop Words:\n",
    "The lemmatization process begins. For each sentence, it goes through each word:\n",
    "Words are lemmatized using the lemmatizer.lemmatize method.\n",
    "Stop words are removed by checking if the lowercase version of the word is not in the set of stop words.\n",
    "\n",
    "Return Lemmatized Sentences:\n",
    "The function returns a list of lemmatized sentences, where each sentence is represented as a list of lemmatized words.\n",
    "\n",
    "This function essentially takes a piece of text, tokenizes it into sentences and words, lemmatizes each word, and removes common English stop words, returning the processed text as a list of lemmatized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Lemmatize each word and remove stop words\n",
    "    lemmatized_sentences = [\n",
    "        [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]\n",
    "        for words in tokenized_sentences\n",
    "    ]\n",
    "\n",
    "    return lemmatized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b92ca63",
   "metadata": {},
   "source": [
    "Lemmatize Text:\n",
    "The input text is lemmatized using the lemmatize_text function, generating a list of lemmatized sentences.\n",
    "\n",
    "Flatten Lemmatized Words:\n",
    "The list of lemmatized sentences is flattened into a single list of all lemmatized words.\n",
    "\n",
    "Calculate Word Frequency:\n",
    "The frequency of each lemmatized word in the text is calculated using the Counter class, creating a dictionary (word_freq) mapping words to their frequencies.\n",
    "\n",
    "Calculate Sentence Importance:\n",
    "For each sentence, an importance score is calculated by summing up the frequencies of its constituent words.\n",
    "\n",
    "Calculate Summary Length:\n",
    "The desired length of the summary is determined based on a percentage of the total number of sentences in the lemmatized text.\n",
    "\n",
    "Select Sentences for Summary:\n",
    "Sentences are selected for the summary based on their importance scores. The sorted function is used to sort indices in descending order of importance.\n",
    "\n",
    "Join Summary Sentences:\n",
    "The selected sentences are joined to form the final summary. Each sentence is joined by a space, and the result is a string representing the summary.\n",
    "\n",
    "Return Summary:\n",
    "The generated summary is returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_length_summary(text, summary_length_percentage=20):\n",
    "    lemmatized_sentences = lemmatize_text(text)\n",
    "\n",
    "    # Flatten the list of lemmatized words\n",
    "    all_words = [word for sentence in lemmatized_sentences for word in sentence]\n",
    "\n",
    "    # Calculate word frequency\n",
    "    word_freq = Counter(all_words)\n",
    "\n",
    "    # Calculate sentence importance based on word frequency\n",
    "    sentence_importance = [sum(word_freq[word] for word in sentence) for sentence in lemmatized_sentences]\n",
    "\n",
    "    # Calculate the total length of the summary in sentences\n",
    "    num_sentences = int(len(lemmatized_sentences) * summary_length_percentage / 100)\n",
    "\n",
    "    # Select sentences based on importance scores for summary\n",
    "    summary_sentences = [lemmatized_sentences[i] for i in sorted(range(len(sentence_importance)), key=lambda k: sentence_importance[k], reverse=True)[:num_sentences]]\n",
    "\n",
    "    # Join the summary sentences to form the final summary\n",
    "    summary = ' '.join([' '.join(sentence) for sentence in summary_sentences])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c762bd",
   "metadata": {},
   "source": [
    "Set Summary Length Percentage:\n",
    "The variable summary_length_percentage is set to 50. This variable represents the desired length of the summary as a percentage of the total number of sentences in the original text.\n",
    "\n",
    "Generate Summary:\n",
    "The variable_length_summary function is called with the input text and the specified summary_length_percentage. The result is stored in the variable summary.\n",
    "\n",
    "Word Count Calculation:\n",
    "The count_words function is used to calculate the number of words in the generated summary. The result is stored in the variable word_count.\n",
    "\n",
    "Display Word Count:\n",
    "The result of the word count calculation is displayed using the print function. It prints a formatted string indicating the number of words in the summary.\n",
    "\n",
    "Display Summary:\n",
    "The generated summary is displayed using the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the summary length percentage (between 1 and 100)\n",
    "summary_length_percentage = 50\n",
    "\n",
    "# Get the summary\n",
    "summary = variable_length_summary(text, summary_length_percentage)\n",
    "\n",
    "# Word Count\n",
    "word_count = count_words(summary)\n",
    "# Display result\n",
    "print(f\"Number of words in the Summary: {word_count}\")\n",
    "\n",
    "# Display the summary\n",
    "print(\"\\nSummary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7002890",
   "metadata": {},
   "source": [
    "Specify Summary Audio File:\n",
    "The variable summary_audio_file is set to the path and filename where the summary audio file will be saved. In this case, it's set to 'Audios/2.Summary.mp3'.\n",
    "\n",
    "Save Summary to Audio File:\n",
    "The engine.save_to_file function is used to save the generated summary to the specified audio file (summary_audio_file). This function converts the text into speech and saves it as an audio file.\n",
    "\n",
    "Run Text-to-Speech Engine:\n",
    "The engine.runAndWait() function is called to execute the text-to-speech engine. This command initiates the generation of the audio file based on the provided summary.\n",
    "\n",
    "Play Audio File:\n",
    "uses the os.system command to play the generated audio file using the default system player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_audio_file = 'Audios/2.Summary.mp3'\n",
    "engine.save_to_file(summary, summary_audio_file)\n",
    "engine.runAndWait()\n",
    "# os.system(f\"start {summary_audio_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f40e380",
   "metadata": {},
   "source": [
    "Instantiate Translator:\n",
    "The function starts by creating an instance of a translation library using the TranslateLib class. This instance is named translator, and it is configured to translate text to a specified target language.\n",
    "\n",
    "Chunking Text:\n",
    "The input text is broken down into smaller chunks of a specified size. This is done using list comprehension, where each chunk is a substring of the original text. The range function is used to iterate over the text in steps of chunk_size.\n",
    "\n",
    "Translate Each Chunk:\n",
    "The function then enters a loop to iterate over each chunk of text. For each chunk, it uses the translator.translate method to obtain the translation of that chunk. The translation is stored in the translation variable.\n",
    "\n",
    "Concatenate Translations:\n",
    "The translated chunks are concatenated together to form the final translated text. The variable translated_text is used to accumulate the translated content from each chunk.\n",
    "\n",
    "Return Translated Text:\n",
    "The function returns the complete translated text, which is the concatenation of translations for all the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text_with_chunking(text, target_language='en', chunk_size=500):\n",
    "    translator = TranslateLib(to_lang=target_language)\n",
    "\n",
    "    # Break down the text into chunks\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "    # Translate each chunk and concatenate the results\n",
    "    translated_text = \"\"\n",
    "    for chunk in chunks:\n",
    "        translation = translator.translate(chunk)\n",
    "        translated_text += translation\n",
    "\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef984a6",
   "metadata": {},
   "source": [
    "Target Language:\n",
    "The target language is set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target languages for translation\n",
    "target_languages = ['hi']  # Hindi and for Gujarati('gu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e0d465",
   "metadata": {},
   "source": [
    "Loop through Target Languages:\n",
    "The code uses a for loop to iterate over each target language specified in the target_languages list.\n",
    "\n",
    "Translate Summary for Each Language:\n",
    "Inside the loop, the translate_text_with_chunking function is called with the summary as input and the current lang as the target language. The result is stored in the variable translated_summary.\n",
    "\n",
    "Display Translated Summary:\n",
    "Prints the translated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the summary to each target language with chunking\n",
    "for lang in target_languages:\n",
    "    translated_summary = translate_text_with_chunking(summary, target_language=lang)\n",
    "\n",
    "    # Display the translated summary\n",
    "    print(f\"\\nTranslated Summary in {lang}:\\n\", translated_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81551c7",
   "metadata": {},
   "source": [
    "The Translated summary will converted to audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb4e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and play the translated summary as audio\n",
    "translated_audio_file = f'Audios/Translated_Summary_To_{lang}.mp3'\n",
    "engine.save_to_file(translated_summary, translated_audio_file)\n",
    "engine.runAndWait()\n",
    "# os.system(f\"start {translated_audio_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
